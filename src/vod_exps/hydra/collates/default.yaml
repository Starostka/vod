predict:
  templates: ${templates} # Templates for the encoder input
  tokenizer: ${tokenizers.encoder}
train:
  # Configuration for the training/evaluation `Collate`
  n_sections: 32 # Number of sections for each query
  max_pos_sections: 8 # Maximum number of positive sections for each query
  support_size: 100 # Maximum number of sections to sample from
  prefetch_n_sections: 128 # Number of sections to fetch from each search service
  do_sample: true # Whether to sample or use the top-k sections
  in_batch_negatives: false # Whether to share sections across all queries in a batch
  # Extras attrs
  query_extras: []
  section_extras: []
  # Base args
  templates: ${templates}
  tokenizer_encoder: ${tokenizers.encoder}
  tokenizer_lm: ${tokenizers.lm}
benchmark:
  # Configuration for the benchmark eval. `Collate`
  n_sections: null
  max_pos_sections: null
  support_size: null
  prefetch_n_sections: 128
  do_sample: false
  in_batch_negatives: false
  prep_num_proc: ${resources.num_proc}
  # Extras attrs
  query_extras: []
  section_extras: []
  # Base args
  templates: ${templates}
  tokenizer_encoder: ${tokenizers.encoder}
  tokenizer_lm: ${tokenizers.lm}
